{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install python-bidi arabic-reshaper -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWc7YYXrsOfE",
        "outputId": "75fac837-ccb7-4370-bfc5-9ecce59d612a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/292.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m286.7/292.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.9/292.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XdTP4oGYYY1a"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import re\n",
        "from bidi.algorithm import get_display\n",
        "import arabic_reshaper\n",
        "from bidi.algorithm import get_display\n",
        "import arabic_reshaper\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/Grad. Project/Etisalat.txt\"\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    html_content = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "def get_main_header(file_path):\n",
        "    \"\"\"\n",
        "    Extracts the main header (e.g., 'اميرالد العيله') from the given HTML file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the HTML file.\n",
        "\n",
        "    Returns:\n",
        "        str or None: The main header text if found, else None.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            html_content = f.read()\n",
        "\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "        # CSS selector mimicking the XPath: /html/body/main/article/section[1]/div/div/div/h1\n",
        "        main_header = soup.select_one(\"main > article > section:nth-of-type(1) .page-title-content h1\")\n",
        "\n",
        "        if main_header:\n",
        "            return main_header.get_text(strip=True)\n",
        "        else:\n",
        "            print(\"Main header not found.\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "def get_description_text(file_path):\n",
        "    \"\"\"\n",
        "    Extracts the description paragraph from the given HTML file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the HTML file.\n",
        "\n",
        "    Returns:\n",
        "        str or None: The description text if found, else None.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            html_content = f.read()\n",
        "\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "        # Navigate using CSS selector mimicking the XPath provided\n",
        "        description = soup.select_one(\"main > article > section:nth-of-type(1) div.container div.row div.page-title-content p.fs-14\")\n",
        "\n",
        "        if description:\n",
        "            return description.get_text(strip=True)\n",
        "        else:\n",
        "            print(\"Description not found.\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Helper to clean and normalize Arabic text.\"\"\"\n",
        "    return re.sub(r'\\s+', ' ', text.strip())\n",
        "\n",
        "def extract_plan_features(file_path):\n",
        "    try:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            html_content = f.read()\n",
        "\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "        target_section = soup.select_one(\"main > article > section:nth-of-type(2) div.container div.tabsContainer .tab-content.show\")\n",
        "        if not target_section:\n",
        "            print(\"Target section not found.\")\n",
        "            return None\n",
        "\n",
        "        plans_data = {}\n",
        "        plan_cards = target_section.select(\".card-container .slick-slide .card\")\n",
        "\n",
        "        for idx, card in enumerate(plan_cards):\n",
        "            plan_name_tag = card.find(\"h5\", class_=\"plan-name\")\n",
        "            plan_price_tag = card.find(\"h5\", class_=\"plan-price\")\n",
        "            plan_items = card.select(\".list-group-item\")\n",
        "\n",
        "            features = []\n",
        "\n",
        "            # Extract price\n",
        "            price = \"\"\n",
        "            if plan_price_tag:\n",
        "                price_text = clean_text(plan_price_tag.get_text())\n",
        "                price_match = re.search(r'(\\d+)(?:\\s|&nbsp;)+جنيه', price_text)\n",
        "                if price_match:\n",
        "                    price = price_match.group(1)\n",
        "\n",
        "            # Add price as first feature\n",
        "            if price:\n",
        "                features.append(f\"السعر : {price} جنيه\")\n",
        "\n",
        "            # Extract other features\n",
        "            for item in plan_items:\n",
        "                label = item.find(\"small\")\n",
        "                value = item.find(\"p\", class_=\"ff-suissintl-bold fs-16\")\n",
        "\n",
        "                if label and value:\n",
        "                    label_text = clean_text(label.get_text()).strip(\":\").strip()\n",
        "                    value_text = clean_text(value.get_text()).strip()\n",
        "                    features.append(f\"{label_text} : {value_text}\")\n",
        "\n",
        "            # Normalize plan name\n",
        "            plan_name = clean_text(plan_name_tag.get_text()) if plan_name_tag else f\"Plan_{idx + 1}\"\n",
        "            plan_name_cleaned = re.sub(r'\\s*\\d+\\s*$', '', plan_name).strip()  # Remove trailing number\n",
        "\n",
        "            # Use full name (with number) as key\n",
        "            plan_key = plan_name\n",
        "\n",
        "            plans_data[plan_key] = features\n",
        "\n",
        "        return plans_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def extract_titles_and_descriptions(file_path):\n",
        "    \"\"\"\n",
        "    Extracts titles and descriptions from the 'Points Program' section.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the HTML file.\n",
        "\n",
        "    Returns:\n",
        "        list of dicts: Each dict contains 'title' and 'description'\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            html_content = f.read()\n",
        "\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "        # Locate the correct section by ID instead of assuming structure\n",
        "        section = soup.find(\"section\", {\"id\": \"for_readAbout\"})\n",
        "        if not section:\n",
        "            print(\"Section 'for_moreProgram' not found.\")\n",
        "            return []\n",
        "\n",
        "        # Find all items inside the slider_1items div\n",
        "        slider_div = section.find(\"div\", class_=\"slider_1items\")\n",
        "        if not slider_div:\n",
        "            print(\"Slider container not found inside the section.\")\n",
        "            return []\n",
        "\n",
        "        items = slider_div.find_all(\"div\", class_=\"item\")\n",
        "        results = []\n",
        "\n",
        "        for item in items:\n",
        "            title_tag = item.find(\"h5\", class_=\"ff-suissintl-bold\")\n",
        "            desc_tags = item.find_all(\"p\")\n",
        "\n",
        "            title = title_tag.get_text(strip=True) if title_tag else None\n",
        "            description = \" \".join([p.get_text(strip=True) for p in desc_tags]) if desc_tags else None\n",
        "\n",
        "            if title or description:\n",
        "                results.append({\n",
        "                    \"title\": title or \"Untitled\",\n",
        "                    \"description\": description or \"No description\"\n",
        "                })\n",
        "\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return []\n",
        "\n",
        "def extract_read_about_section(file_path):\n",
        "    \"\"\"\n",
        "    Extracts the 'Read About' section (id='for_readAbout') from the HTML file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the HTML file.\n",
        "\n",
        "    Returns:\n",
        "        list of dicts: Each dict contains 'title', 'description', and 'logo_url'\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            html_content = f.read()\n",
        "\n",
        "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "\n",
        "        # Locate the correct section by ID\n",
        "        section = soup.find(\"section\", {\"id\": \"for_readAbout\"})\n",
        "        if not section:\n",
        "            print(\"Section 'for_readAbout' not found.\")\n",
        "            return []\n",
        "\n",
        "        # Find the grid container that holds all items\n",
        "        grid_container = section.find(\"div\", class_=\"grid\")\n",
        "        if not grid_container:\n",
        "            print(\"Grid container not found inside 'for_readAbout'.\")\n",
        "            return []\n",
        "\n",
        "        items = grid_container.find_all(\"div\", recursive=False)\n",
        "        results = []\n",
        "\n",
        "        for item in items:\n",
        "            logo_img = item.find(\"img\")\n",
        "            title_tag = item.find(\"p\", class_=\"my-2\")\n",
        "            desc_tag = item.find_next(\"p\", class_=lambda x: x and \"grey-2-color\" in x)\n",
        "\n",
        "            logo_url = logo_img[\"src\"] if logo_img and \"src\" in logo_img.attrs else None\n",
        "            title = title_tag.get_text(strip=True) if title_tag else \"Untitled\"\n",
        "            description = desc_tag.get_text(strip=True) if desc_tag else \"No description\"\n",
        "\n",
        "            results.append({\n",
        "                \"title\": title,\n",
        "                \"description\": description,\n",
        "                \"logo_url\": logo_url\n",
        "            })\n",
        "\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def extract_gto_emerald_offer_section(file_path):\n",
        "    \"\"\"\n",
        "    Extracts the 'عروض GTO وإي آند مصراميرالد' section from the HTML file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the HTML file.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing 'title' and 'description' if found, else None.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            html_content = f.read()\n",
        "\n",
        "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "\n",
        "        # Try to find the title by partial match\n",
        "        title_tag = None\n",
        "        for tag in soup.find_all([\"h5\", \"h6\"]):\n",
        "            if \"عروض GTO\" in tag.get_text(strip=True) and \"إي آند مصراميرالد\" in tag.get_text(strip=True):\n",
        "                title_tag = tag\n",
        "                break\n",
        "\n",
        "        if not title_tag:\n",
        "            print(\"Section title not found.\")\n",
        "            return None\n",
        "\n",
        "        # Find the next paragraph that contains meaningful Arabic text\n",
        "        desc_tag = None\n",
        "        for sibling in title_tag.find_next_siblings():\n",
        "            if sibling.name == \"p\":\n",
        "                desc_text = sibling.get_text(strip=True)\n",
        "                if len(desc_text) > 20:  # Ensure it's not empty or too short\n",
        "                    desc_tag = sibling\n",
        "                    break\n",
        "\n",
        "        if not desc_tag:\n",
        "            print(\"Description not found.\")\n",
        "            return None\n",
        "\n",
        "        # Extract and clean the text\n",
        "        title = \" \".join(title_tag.get_text(strip=True).split())\n",
        "        description = \" \".join(desc_tag.get_text(strip=True).split())\n",
        "\n",
        "        return {\n",
        "            \"title\": title,\n",
        "            \"description\": description\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def extract_family_section(file_path):\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    Extracts the 'العائله' (Family) section from the HTML file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the HTML file.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing 'title', 'description', and 'image_url'\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            html_content = f.read()\n",
        "\n",
        "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "\n",
        "        # Try to find the card that contains the family section\n",
        "        family_card = None\n",
        "        for card in soup.find_all(\"div\", class_=\"card\"):\n",
        "            title_tag = card.find(\"h5\", class_=\"ff-suissintl-semi-bold\")\n",
        "            if title_tag and \"العائله\" in title_tag.get_text(strip=True):\n",
        "                family_card = card\n",
        "                break\n",
        "\n",
        "        if not family_card:\n",
        "            print(\"Family section not found.\")\n",
        "            return None\n",
        "\n",
        "        # Extract title and clean it\n",
        "        title_tag = family_card.find(\"h5\", class_=\"ff-suissintl-semi-bold\")\n",
        "        title = title_tag.get_text(strip=True) if title_tag else \"العائله\"\n",
        "        title = \" \".join(title.split())  # Clean extra spaces\n",
        "\n",
        "        # Extract description and clean it\n",
        "        desc_tag = family_card.find(\"p\", class_=\"fs-16\")\n",
        "        description = desc_tag.get_text(strip=True) if desc_tag else None\n",
        "        if description:\n",
        "            description = \" \".join(description.split())  # Clean extra spaces\n",
        "\n",
        "        return {\n",
        "            \"title\": title,\n",
        "            \"description\": description,\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def extract_exclusive_privileges_section(file_path):\n",
        "    \"\"\"\n",
        "    Extracts the 'اميرالد Exclusive Privileges' section from the HTML file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the HTML file.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing 'title', 'description', and 'image_url'\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            html_content = f.read()\n",
        "\n",
        "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "\n",
        "        # Find the card that contains the exclusive privileges section\n",
        "        exclusive_card = None\n",
        "        for card in soup.find_all(\"div\", class_=\"card\"):\n",
        "            title_tag = card.find(\"h5\", class_=\"ff-suissintl-semi-bold\")\n",
        "            if title_tag and \"Exclusive Privileges\" in title_tag.get_text(strip=True):\n",
        "                exclusive_card = card\n",
        "                break\n",
        "\n",
        "        if not exclusive_card:\n",
        "            print(\"Exclusive Privileges section not found.\")\n",
        "            return None\n",
        "\n",
        "        # Extract title and clean it\n",
        "        title_tag = exclusive_card.find(\"h5\", class_=\"ff-suissintl-semi-bold\")\n",
        "        title = title_tag.get_text(strip=True) if title_tag else \"اميرالد Exclusive Privileges\"\n",
        "        title = \" \".join(title.split())  # Clean extra spaces\n",
        "\n",
        "        # Extract description and clean it\n",
        "        desc_tags = exclusive_card.find_all(\"p\", class_=\"fs-16\")\n",
        "        description = \"\"\n",
        "        for p in desc_tags:\n",
        "            p_text = p.get_text(strip=True)\n",
        "            if p_text:\n",
        "                description += p_text + \" \"\n",
        "        description = \" \".join(description.strip().split())  # Clean whitespace\n",
        "\n",
        "        return {\n",
        "            \"title\": title,\n",
        "            \"description\": description,\n",
        "\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def extract_entertainment_experience_section(file_path):\n",
        "    \"\"\"\n",
        "    Extracts the 'مع تجربه الترفيهيه' section from the HTML file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the HTML file.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing 'title', 'description', and 'image_url'\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            html_content = f.read()\n",
        "\n",
        "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "\n",
        "        # Find the card that contains the entertainment experience section\n",
        "        experience_card = None\n",
        "        for card in soup.find_all(\"div\", class_=\"card\"):\n",
        "            title_tag = card.find(\"h5\", class_=\"ff-suissintl-semi-bold\")\n",
        "            if title_tag and \"تجربه الترفيهيه\" in title_tag.get_text(strip=True):\n",
        "                experience_card = card\n",
        "                break\n",
        "\n",
        "        if not experience_card:\n",
        "            print(\"Entertainment experience section not found.\")\n",
        "            return None\n",
        "\n",
        "        # Extract title and clean it\n",
        "        title_tag = experience_card.find(\"h5\", class_=\"ff-suissintl-semi-bold\")\n",
        "        title = title_tag.get_text(strip=True) if title_tag else \"مع تجربه الترفيهيه\"\n",
        "        title = \" \".join(title.split())  # Clean extra spaces\n",
        "\n",
        "        # Extract description and clean it\n",
        "        desc_tag = experience_card.find(\"p\", class_=\"\")\n",
        "        description = desc_tag.get_text(strip=True) if desc_tag else None\n",
        "        if description:\n",
        "            description = \" \".join(desc_tag.get_text(strip=True).split())  # Clean extra spaces\n",
        "\n",
        "        return {\n",
        "            \"title\": title,\n",
        "            \"description\": description,\n",
        "\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def extract_terms_and_conditions_section(file_path):\n",
        "    \"\"\"\n",
        "    Extracts the 'الشروط و الأحكام' (Terms and Conditions) section from the HTML file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the HTML file.\n",
        "\n",
        "    Returns:\n",
        "        list of dicts: Each dict contains 'heading' and 'content' if found\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            html_content = f.read()\n",
        "\n",
        "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "\n",
        "        # Find the main terms section by ID or structure\n",
        "        terms_section = soup.find(\"section\", {\"id\": \"for_features_and_terms\"})\n",
        "        if not terms_section:\n",
        "            print(\"Terms & Conditions section not found.\")\n",
        "            return []\n",
        "\n",
        "        container = terms_section.find(\"div\", {\"id\": \"Terms\"})\n",
        "        if not container:\n",
        "            print(\"Terms container not found.\")\n",
        "            return []\n",
        "\n",
        "        # Extract all meaningful blocks inside the container\n",
        "        content_blocks = container.find_all([\"h3\", \"h4\", \"h5\", \"p\", \"ul\", \"li\"])\n",
        "\n",
        "        results = []\n",
        "        current_heading = None\n",
        "\n",
        "        for block in content_blocks:\n",
        "            if block.name in [\"h3\", \"h4\", \"h5\"]:\n",
        "                # New heading found\n",
        "                heading_text = \" \".join(block.stripped_strings)\n",
        "                if heading_text:\n",
        "                    current_heading = heading_text\n",
        "            elif block.name == \"p\":\n",
        "                paragraph = \" \".join(block.stripped_strings)\n",
        "                if paragraph:\n",
        "                    results.append({\n",
        "                        \"heading\": current_heading or \"ملاحظات\",\n",
        "                        \"content\": paragraph\n",
        "                    })\n",
        "            elif block.name == \"li\":\n",
        "                list_item = \" \".join(block.stripped_strings)\n",
        "                if list_item:\n",
        "                    results.append({\n",
        "                        \"heading\": current_heading or \"قائمة\",\n",
        "                        \"content\": list_item\n",
        "                    })\n",
        "            elif block.name == \"ul\":\n",
        "                list_items = block.find_all(\"li\")\n",
        "                for li in list_items:\n",
        "                    item_text = \" \".join(li.stripped_strings)\n",
        "                    if item_text:\n",
        "                        results.append({\n",
        "                            \"heading\": current_heading or \"قائمة\",\n",
        "                            \"content\": item_text\n",
        "                        })\n",
        "\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return []\n",
        "\n"
      ],
      "metadata": {
        "id": "NPzZl7PjZvFP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_all_extraction(file_path):\n",
        "    data = {}\n",
        "\n",
        "    # Run each extraction function and store result\n",
        "    try:\n",
        "        main_header = get_main_header(file_path)\n",
        "        if main_header:\n",
        "            data[\"main_header\"] = fix_rtl_text(main_header)\n",
        "    except Exception as e:\n",
        "        print(\"Error extracting main header:\", e)\n",
        "\n",
        "    try:\n",
        "        description = get_description_text(file_path)\n",
        "        if description:\n",
        "            data[\"description\"] = fix_rtl_text(description)\n",
        "    except Exception as e:\n",
        "        print(\"Error extracting description:\", e)\n",
        "\n",
        "    try:\n",
        "        gto_offer = extract_gto_emerald_offer_section(file_path)\n",
        "        if gto_offer:\n",
        "            data[\"gto_emerald_offer\"] = {\n",
        "                \"title\": fix_rtl_text(gto_offer[\"title\"]),\n",
        "                \"description\": fix_rtl_text(gto_offer[\"description\"])\n",
        "            }\n",
        "    except Exception as e:\n",
        "        print(\"Error extracting GTO Emerald Offer:\", e)\n",
        "\n",
        "    try:\n",
        "        family_section = extract_family_section(file_path)\n",
        "        if family_section:\n",
        "            data[\"family_section\"] = {\n",
        "                \"title\": fix_rtl_text(family_section[\"title\"]),\n",
        "                \"description\": fix_rtl_text(family_section[\"description\"]),\n",
        "                \"image_url\": family_section.get(\"image_url\")\n",
        "            }\n",
        "    except Exception as e:\n",
        "        print(\"Error extracting Family section:\", e)\n",
        "\n",
        "    try:\n",
        "        entertainment = extract_entertainment_experience_section(file_path)\n",
        "        if entertainment:\n",
        "            data[\"entertainment_experience\"] = {\n",
        "                \"title\": fix_rtl_text(entertainment[\"title\"]),\n",
        "                \"description\": fix_rtl_text(entertainment[\"description\"]),\n",
        "                \"image_url\": entertainment.get(\"image_url\")\n",
        "            }\n",
        "    except Exception as e:\n",
        "        print(\"Error extracting Entertainment Experience:\", e)\n",
        "\n",
        "    try:\n",
        "        terms = extract_terms_and_conditions_section(file_path)\n",
        "        cleaned_terms = [\n",
        "            {\n",
        "                \"heading\": fix_rtl_text(t[\"heading\"]),\n",
        "                \"content\": fix_rtl_text(t[\"content\"])\n",
        "            } for t in terms if t[\"heading\"] and t[\"content\"]\n",
        "        ]\n",
        "        if cleaned_terms:\n",
        "            data[\"terms_and_conditions\"] = cleaned_terms\n",
        "    except Exception as e:\n",
        "        print(\"Error extracting Terms & Conditions:\", e)\n",
        "\n",
        "    try:\n",
        "        plan_features = extract_plan_features(file_path)\n",
        "        if plan_features:\n",
        "            data[\"plan_features\"] = plan_features\n",
        "    except Exception as e:\n",
        "        print(\"Error extracting Plan Features:\", e)\n",
        "\n",
        "    try:\n",
        "        points_program = extract_points_program_section(file_path)\n",
        "        if points_program:\n",
        "            data[\"points_program\"] = {\n",
        "                \"title\": fix_rtl_text(points_program[\"title\"]),\n",
        "                \"description\": fix_rtl_text(points_program[\"description\"]),\n",
        "                \"image_url\": points_program.get(\"image_url\"),\n",
        "                \"more_info_link\": points_program.get(\"more_info_link\")\n",
        "            }\n",
        "    except Exception as e:\n",
        "        print(\"Error extracting Points Program:\", e)\n",
        "\n",
        "    try:\n",
        "        exclusive_privileges = extract_exclusive_privileges_section(file_path)\n",
        "        if exclusive_privileges:\n",
        "            data[\"exclusive_privileges\"] = {\n",
        "                \"title\": fix_rtl_text(exclusive_privileges[\"title\"]),\n",
        "                \"description\": fix_rtl_text(exclusive_privileges[\"description\"])\n",
        "            }\n",
        "    except Exception as e:\n",
        "        print(\"Error extracting Exclusive Privileges:\", e)\n",
        "\n",
        "    try:\n",
        "        read_about = extract_read_about_section(file_path)\n",
        "        if read_about:\n",
        "            data[\"read_about_section\"] = [\n",
        "                {\n",
        "                    \"title\": fix_rtl_text(item[\"title\"]),\n",
        "                    \"description\": fix_rtl_text(item[\"description\"])\n",
        "                } for item in read_about\n",
        "            ]\n",
        "    except Exception as e:\n",
        "        print(\"Error extracting Read About Section:\", e)\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "21FZZ3bCra4s"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    file_path = \"/content/drive/MyDrive/Grad. Project/Etisalat.txt\"\n",
        "\n",
        "    final_data = run_all_extraction(file_path)\n",
        "\n",
        "    output_file = \"emerald_data.json\"\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(final_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"✅ Extraction complete. Saved to '{output_file}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aEi2HCmbMC8",
        "outputId": "e2fc1c06-5c61-4ca3-ab29-879bfe303aeb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error extracting main header: name 'fix_rtl_text' is not defined\n",
            "Error extracting description: name 'fix_rtl_text' is not defined\n",
            "Error extracting GTO Emerald Offer: name 'fix_rtl_text' is not defined\n",
            "Error extracting Family section: name 'fix_rtl_text' is not defined\n",
            "Error extracting Entertainment Experience: name 'fix_rtl_text' is not defined\n",
            "Error extracting Terms & Conditions: name 'fix_rtl_text' is not defined\n",
            "Error extracting Points Program: name 'extract_points_program_section' is not defined\n",
            "Error extracting Exclusive Privileges: name 'fix_rtl_text' is not defined\n",
            "Grid container not found inside 'for_readAbout'.\n",
            "✅ Extraction complete. Saved to 'emerald_data.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def fix_rtl_text(text):\n",
        "    if not text:\n",
        "        return text\n",
        "    # Reshape Arabic letters\n",
        "    reshaped_text = arabic_reshaper.reshape(text)\n",
        "    # Apply BiDi algorithm\n",
        "    return get_display(reshaped_text)\n",
        "\n",
        "def fix_nested_dict(d):\n",
        "    for key, value in d.items():\n",
        "        if isinstance(value, dict):\n",
        "            fix_nested_dict(value)\n",
        "        elif isinstance(value, list):\n",
        "            for item in value:\n",
        "                if isinstance(item, dict):\n",
        "                    fix_nested_dict(item)\n",
        "                elif isinstance(item, str):\n",
        "                    idx = value.index(item)\n",
        "                    value[idx] = fix_rtl_text(item)\n",
        "        elif isinstance(value, str):\n",
        "            d[key] = fix_rtl_text(value)\n",
        "    return d\n",
        "\n",
        "# Load your existing JSON file\n",
        "input_file = \"/content/drive/MyDrive/Grad. Project/emerald_data.json\"\n",
        "output_file = \"emerald_data_fixed.json\"\n",
        "\n",
        "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Fix RTL display issues\n",
        "fixed_data = fix_nested_dict(data)\n",
        "\n",
        "# Save fixed JSON\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(fixed_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"✅ Fixed JSON saved to '{output_file}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYLIUC24lEhy",
        "outputId": "40df9d37-4fbc-4c6d-92f7-712cc84eb853"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fixed JSON saved to 'emerald_data_fixed.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Fix"
      ],
      "metadata": {
        "id": "jKmbsdEPhLiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import json\n",
        "# import re\n",
        "\n",
        "# # Load the original JSON file\n",
        "# with open('/content/drive/MyDrive/Grad. Project/emerald_data.json', 'r', encoding='utf-8') as f:\n",
        "#     data = json.load(f)\n",
        "\n",
        "# # Cleaning and reordering function\n",
        "# def clean_and_fix_mixed_arabic(text):\n",
        "#     # Remove newlines and extra spaces\n",
        "#     text = re.sub(r'\\n+', ' ', text)\n",
        "#     text = re.sub(r'\\s{2,}', ' ', text).strip()\n",
        "\n",
        "#     # Fix missing space after colons (like \"VIP:استمتع\" => \"VIP: استمتع\")\n",
        "#     text = re.sub(r'([:\\u061F])([^\\s])', r'\\1 \\2', text)\n",
        "\n",
        "#     # Fix specific: \"خصومات الفنادق من <EN> : <VAL>\" => \"<VAL> : <EN> خصومات الفنادق من\"\n",
        "#     match_discount = re.search(r'(خصومات الفنادق من)\\s+([A-Za-z0-9 %]+)\\s*[:：]\\s*([^\\n]+)', text)\n",
        "#     if match_discount:\n",
        "#         return f\"{match_discount.group(3)} : {match_discount.group(2)} {match_discount.group(1)}\"\n",
        "\n",
        "#     # General: \"<Arabic> <English words> : <value>\" => \"<value> : <English> <Arabic>\"\n",
        "#     match_general = re.search(r'([اأإء-ي]+\\s+)((?:[A-Za-z0-9]+\\s?)+)\\s*[:：]\\s*([^\\n]+)', text)\n",
        "#     if match_general:\n",
        "#         arabic = match_general.group(1).strip()\n",
        "#         english = match_general.group(2).strip()\n",
        "#         value = match_general.group(3).strip()\n",
        "#         return f\"{value} : {english} {arabic}\"\n",
        "\n",
        "#     # Fix encoding artifacts\n",
        "#     text = text.replace(\"ﺍﻝ ﺇﻟﻰ\", \"إلى\")\n",
        "#     text = text.replace(\"ﺍﻝ\", \"ال\")\n",
        "#     text = text.replace(\"  \", \" \")\n",
        "\n",
        "#     return text\n",
        "\n",
        "# # Apply to plan features\n",
        "# for plan, features in data.get(\"plan_features\", {}).items():\n",
        "#     data[\"plan_features\"][plan] = [clean_and_fix_mixed_arabic(f) for f in features]\n",
        "\n",
        "# # Apply to terms and conditions\n",
        "# for item in data.get(\"terms_and_conditions\", []):\n",
        "#     item[\"content\"] = clean_and_fix_mixed_arabic(item[\"content\"])\n",
        "\n",
        "# # Apply to sections with descriptions\n",
        "# for section in [\"gto_emerald_offer\", \"family_section\", \"exclusive_privileges\"]:\n",
        "#     if section in data and \"description\" in data[section]:\n",
        "#         data[section][\"description\"] = clean_and_fix_mixed_arabic(data[section][\"description\"])\n",
        "\n",
        "# # Fix gto_emerald_offer title\n",
        "# if \"gto_emerald_offer\" in data:\n",
        "#     data[\"gto_emerald_offer\"][\"title\"] = \"عروض GTO من Emerald و e& مصر\"\n",
        "\n",
        "# # Save final cleaned file\n",
        "# with open(\"emerald_data_packages.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "#     json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# print(\"✅ JSON cleaned and saved with all improvements.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Od5hznGEeZws",
        "outputId": "7fdcdca6-0bab-4510-8283-6b40ac13e7bf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ JSON cleaned and saved with all improvements.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_and_fix_mixed_arabic(text):\n",
        "    # Remove newlines and extra spaces\n",
        "    text = re.sub(r'\\n+', ' ', text)\n",
        "    text = re.sub(r'\\s{2,}', ' ', text).strip()\n",
        "\n",
        "    # Fix missing space after colon (e.g. \"VIP:استمتع\")\n",
        "    text = re.sub(r'([:\\u061F])([^\\s])', r'\\1 \\2', text)\n",
        "\n",
        "    # Fix encoding artifacts\n",
        "    text = text.replace(\"ﺍﻝ\", \"ال\")\n",
        "    text = text.replace(\"ﻛﺎﻻﺗﻲ\", \"كما يلي\")\n",
        "    text = text.replace(\"ﺍﻷﻧﺘﺮﻧﺖ\", \"الإنترنت\")\n",
        "    text = text.replace(\"ﺑﺎﻗﺔ\", \"باقة\")\n",
        "    text = text.replace(\"ﺍﻟﻤﺤﻠﻴﺔ\", \"المحلية\")\n",
        "    text = text.replace(\"ﺧﺪﻣﺔ\", \"خدمة\")\n",
        "    text = text.replace(\"ﻟﻼﺳﺘﻤﺘﺎﻉ\", \"للاستمتاع\")\n",
        "    text = text.replace(\"ﺧﺼﻢ\", \"خصم\")\n",
        "    text = text.replace(\"ﺟﻴﺠﺎﺑﻴﺖ\", \"الجيجابايت\")\n",
        "\n",
        "    # Special fix: \"ال eHome DSL\" → \"خدمة eHome DSL\"\n",
        "    text = re.sub(r'ال\\s+eHome DSL', \"خدمة eHome DSL\", text)\n",
        "\n",
        "    # 1. خصومات الفنادق من ...\n",
        "    match_discount = re.search(r'(خصومات الفنادق من)\\s+([A-Za-z0-9 %]+)\\s*[:：]\\s*([^\\n]+)', text)\n",
        "    if match_discount:\n",
        "        return f\"{match_discount.group(3)} : {match_discount.group(2)} {match_discount.group(1)}\"\n",
        "\n",
        "    # 2. \"<Arabic> <English words> : <value>\" → \"<value> : <English> <Arabic>\"\n",
        "    match_general = re.search(r'([اأإء-ي]+\\s+)((?:[A-Za-z0-9]+\\s?)+)\\s*[:：]\\s*([^\\n]+)', text)\n",
        "    if match_general:\n",
        "        arabic = match_general.group(1).strip()\n",
        "        english = match_general.group(2).strip()\n",
        "        value = match_general.group(3).strip()\n",
        "        return f\"{value} : {english} {arabic}\"\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply to plan features\n",
        "for plan, features in data.get(\"plan_features\", {}).items():\n",
        "    data[\"plan_features\"][plan] = [clean_and_fix_mixed_arabic(f) for f in features]\n",
        "\n",
        "# Apply to terms and conditions\n",
        "for item in data.get(\"terms_and_conditions\", []):\n",
        "    item[\"content\"] = clean_and_fix_mixed_arabic(item[\"content\"])\n",
        "\n",
        "# Apply to sections with descriptions\n",
        "for section in [\"gto_emerald_offer\", \"family_section\", \"exclusive_privileges\"]:\n",
        "    if section in data and \"description\" in data[section]:\n",
        "        data[section][\"description\"] = clean_and_fix_mixed_arabic(data[section][\"description\"])\n",
        "\n",
        "# Fix gto_emerald_offer title\n",
        "if \"gto_emerald_offer\" in data:\n",
        "    data[\"gto_emerald_offer\"][\"title\"] = \"عروض GTO من Emerald و e& مصر\"\n",
        "\n",
        "# Save final cleaned file\n",
        "with open(\"emerald_data_packages.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"✅ JSON cleaned and saved with all improvements.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZAdIT3zf9mC",
        "outputId": "0e5b52b0-0956-462f-966a-ba600beb77ef"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ JSON cleaned and saved with all improvements.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xYaqcoYshsPC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}