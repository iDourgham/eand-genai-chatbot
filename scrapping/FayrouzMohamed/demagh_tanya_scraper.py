# -*- coding: utf-8 -*-
"""demagh_tanya_scraper

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HnD3WCUImpBWQisaJurOs8JpjiLpuISE
"""

from bs4 import BeautifulSoup
import json

# Load the HTML content
with open("demagh_tanya.html", "r", encoding="utf-8") as f:
    html = f.read()

soup = BeautifulSoup(html, "html.parser")

# Find the table with all the plan data
table = soup.select_one("table.table")

# Extract plan titles (they're in the first row)
header_row = table.select_one("tr")
plan_titles = [h5.get_text(strip=True) for h5 in header_row.find_all("h5")]

# Define the row labels we'll map from (6 rows only)
row_labels = [
    "data",
    "units",
    "apps",
    "extra_services",
    "extra_subscriptions"
]

# Get the actual table rows (skip the header)
rows = table.select("tr")[1:7]  # we only want the 6 feature rows

# Prepare a column-wise structure: each column is a plan
columns = list(zip(*[
    [td.get_text(strip=True) for td in row.find_all("td")]
    for row in rows
]))

# Create a dictionary with cleaned format
plans_data = {}
for i, title in enumerate(plan_titles):
    plan_id = f"plan_{i+1}"
    plans_data[plan_id] = {
        "name": title,
        "data": columns[i][0],
        "units": columns[i][1],
        "apps": columns[i][2],
        "extra_services": columns[i][3],
        "extra_subscriptions": columns[i][4],
    }

# Save to JSON
with open("demagh_tanya.json", "w", encoding="utf-8") as f:
    json.dump(plans_data, f, ensure_ascii=False, indent=2)

print("âœ… Data successfully saved to demagh_tanya.json")